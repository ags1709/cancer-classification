{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from copy import copy \n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-1.373633e-16</td>\n",
       "      <td>6.868164e-17</td>\n",
       "      <td>-1.248757e-16</td>\n",
       "      <td>-2.185325e-16</td>\n",
       "      <td>-8.366672e-16</td>\n",
       "      <td>1.873136e-16</td>\n",
       "      <td>4.995028e-17</td>\n",
       "      <td>-4.995028e-17</td>\n",
       "      <td>1.748260e-16</td>\n",
       "      <td>4.745277e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.241796e-16</td>\n",
       "      <td>1.248757e-17</td>\n",
       "      <td>-3.746271e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.372638e-16</td>\n",
       "      <td>-3.371644e-16</td>\n",
       "      <td>7.492542e-17</td>\n",
       "      <td>2.247763e-16</td>\n",
       "      <td>2.622390e-16</td>\n",
       "      <td>-5.744282e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "      <td>1.000880e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.029648e+00</td>\n",
       "      <td>-2.229249e+00</td>\n",
       "      <td>-1.984504e+00</td>\n",
       "      <td>-1.454443e+00</td>\n",
       "      <td>-3.112085e+00</td>\n",
       "      <td>-1.610136e+00</td>\n",
       "      <td>-1.114873e+00</td>\n",
       "      <td>-1.261820e+00</td>\n",
       "      <td>-2.744117e+00</td>\n",
       "      <td>-1.819865e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.726901e+00</td>\n",
       "      <td>-2.223994e+00</td>\n",
       "      <td>-1.693361e+00</td>\n",
       "      <td>-1.222423</td>\n",
       "      <td>-2.682695e+00</td>\n",
       "      <td>-1.443878e+00</td>\n",
       "      <td>-1.305831e+00</td>\n",
       "      <td>-1.745063e+00</td>\n",
       "      <td>-2.160960e+00</td>\n",
       "      <td>-1.601839e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.893853e-01</td>\n",
       "      <td>-7.259631e-01</td>\n",
       "      <td>-6.919555e-01</td>\n",
       "      <td>-6.671955e-01</td>\n",
       "      <td>-7.109628e-01</td>\n",
       "      <td>-7.470860e-01</td>\n",
       "      <td>-7.437479e-01</td>\n",
       "      <td>-7.379438e-01</td>\n",
       "      <td>-7.032397e-01</td>\n",
       "      <td>-7.226392e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.749213e-01</td>\n",
       "      <td>-7.486293e-01</td>\n",
       "      <td>-6.895783e-01</td>\n",
       "      <td>-0.642136</td>\n",
       "      <td>-6.912304e-01</td>\n",
       "      <td>-6.810833e-01</td>\n",
       "      <td>-7.565142e-01</td>\n",
       "      <td>-7.563999e-01</td>\n",
       "      <td>-6.418637e-01</td>\n",
       "      <td>-6.919118e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.150816e-01</td>\n",
       "      <td>-1.046362e-01</td>\n",
       "      <td>-2.359800e-01</td>\n",
       "      <td>-2.951869e-01</td>\n",
       "      <td>-3.489108e-02</td>\n",
       "      <td>-2.219405e-01</td>\n",
       "      <td>-3.422399e-01</td>\n",
       "      <td>-3.977212e-01</td>\n",
       "      <td>-7.162650e-02</td>\n",
       "      <td>-1.782793e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.690395e-01</td>\n",
       "      <td>-4.351564e-02</td>\n",
       "      <td>-2.859802e-01</td>\n",
       "      <td>-0.341181</td>\n",
       "      <td>-4.684277e-02</td>\n",
       "      <td>-2.695009e-01</td>\n",
       "      <td>-2.182321e-01</td>\n",
       "      <td>-2.234689e-01</td>\n",
       "      <td>-1.274095e-01</td>\n",
       "      <td>-2.164441e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.693926e-01</td>\n",
       "      <td>5.841756e-01</td>\n",
       "      <td>4.996769e-01</td>\n",
       "      <td>3.635073e-01</td>\n",
       "      <td>6.361990e-01</td>\n",
       "      <td>4.938569e-01</td>\n",
       "      <td>5.260619e-01</td>\n",
       "      <td>6.469351e-01</td>\n",
       "      <td>5.307792e-01</td>\n",
       "      <td>4.709834e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>5.220158e-01</td>\n",
       "      <td>6.583411e-01</td>\n",
       "      <td>5.402790e-01</td>\n",
       "      <td>0.357589</td>\n",
       "      <td>5.975448e-01</td>\n",
       "      <td>5.396688e-01</td>\n",
       "      <td>5.311411e-01</td>\n",
       "      <td>7.125100e-01</td>\n",
       "      <td>4.501382e-01</td>\n",
       "      <td>4.507624e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.971288e+00</td>\n",
       "      <td>4.651889e+00</td>\n",
       "      <td>3.976130e+00</td>\n",
       "      <td>5.250529e+00</td>\n",
       "      <td>4.770911e+00</td>\n",
       "      <td>4.568425e+00</td>\n",
       "      <td>4.243589e+00</td>\n",
       "      <td>3.927930e+00</td>\n",
       "      <td>4.484751e+00</td>\n",
       "      <td>4.910919e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.094189e+00</td>\n",
       "      <td>3.885905e+00</td>\n",
       "      <td>4.287337e+00</td>\n",
       "      <td>5.930172</td>\n",
       "      <td>3.955374e+00</td>\n",
       "      <td>5.112877e+00</td>\n",
       "      <td>4.700669e+00</td>\n",
       "      <td>2.685877e+00</td>\n",
       "      <td>6.046041e+00</td>\n",
       "      <td>6.846856e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  5.690000e+02  5.690000e+02  5.690000e+02  5.690000e+02  5.690000e+02   \n",
       "mean  -1.373633e-16  6.868164e-17 -1.248757e-16 -2.185325e-16 -8.366672e-16   \n",
       "std    1.000880e+00  1.000880e+00  1.000880e+00  1.000880e+00  1.000880e+00   \n",
       "min   -2.029648e+00 -2.229249e+00 -1.984504e+00 -1.454443e+00 -3.112085e+00   \n",
       "25%   -6.893853e-01 -7.259631e-01 -6.919555e-01 -6.671955e-01 -7.109628e-01   \n",
       "50%   -2.150816e-01 -1.046362e-01 -2.359800e-01 -2.951869e-01 -3.489108e-02   \n",
       "75%    4.693926e-01  5.841756e-01  4.996769e-01  3.635073e-01  6.361990e-01   \n",
       "max    3.971288e+00  4.651889e+00  3.976130e+00  5.250529e+00  4.770911e+00   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  5.690000e+02  5.690000e+02  5.690000e+02  5.690000e+02  5.690000e+02   \n",
       "mean   1.873136e-16  4.995028e-17 -4.995028e-17  1.748260e-16  4.745277e-16   \n",
       "std    1.000880e+00  1.000880e+00  1.000880e+00  1.000880e+00  1.000880e+00   \n",
       "min   -1.610136e+00 -1.114873e+00 -1.261820e+00 -2.744117e+00 -1.819865e+00   \n",
       "25%   -7.470860e-01 -7.437479e-01 -7.379438e-01 -7.032397e-01 -7.226392e-01   \n",
       "50%   -2.219405e-01 -3.422399e-01 -3.977212e-01 -7.162650e-02 -1.782793e-01   \n",
       "75%    4.938569e-01  5.260619e-01  6.469351e-01  5.307792e-01  4.709834e-01   \n",
       "max    4.568425e+00  4.243589e+00  3.927930e+00  4.484751e+00  4.910919e+00   \n",
       "\n",
       "       ...            20            21            22          23  \\\n",
       "count  ...  5.690000e+02  5.690000e+02  5.690000e+02  569.000000   \n",
       "mean   ... -8.241796e-16  1.248757e-17 -3.746271e-16    0.000000   \n",
       "std    ...  1.000880e+00  1.000880e+00  1.000880e+00    1.000880   \n",
       "min    ... -1.726901e+00 -2.223994e+00 -1.693361e+00   -1.222423   \n",
       "25%    ... -6.749213e-01 -7.486293e-01 -6.895783e-01   -0.642136   \n",
       "50%    ... -2.690395e-01 -4.351564e-02 -2.859802e-01   -0.341181   \n",
       "75%    ...  5.220158e-01  6.583411e-01  5.402790e-01    0.357589   \n",
       "max    ...  4.094189e+00  3.885905e+00  4.287337e+00    5.930172   \n",
       "\n",
       "                 24            25            26            27            28  \\\n",
       "count  5.690000e+02  5.690000e+02  5.690000e+02  5.690000e+02  5.690000e+02   \n",
       "mean  -2.372638e-16 -3.371644e-16  7.492542e-17  2.247763e-16  2.622390e-16   \n",
       "std    1.000880e+00  1.000880e+00  1.000880e+00  1.000880e+00  1.000880e+00   \n",
       "min   -2.682695e+00 -1.443878e+00 -1.305831e+00 -1.745063e+00 -2.160960e+00   \n",
       "25%   -6.912304e-01 -6.810833e-01 -7.565142e-01 -7.563999e-01 -6.418637e-01   \n",
       "50%   -4.684277e-02 -2.695009e-01 -2.182321e-01 -2.234689e-01 -1.274095e-01   \n",
       "75%    5.975448e-01  5.396688e-01  5.311411e-01  7.125100e-01  4.501382e-01   \n",
       "max    3.955374e+00  5.112877e+00  4.700669e+00  2.685877e+00  6.046041e+00   \n",
       "\n",
       "                 29  \n",
       "count  5.690000e+02  \n",
       "mean  -5.744282e-16  \n",
       "std    1.000880e+00  \n",
       "min   -1.601839e+00  \n",
       "25%   -6.919118e-01  \n",
       "50%   -2.164441e-01  \n",
       "75%    4.507624e-01  \n",
       "max    6.846856e+00  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/Cancer_data.csv\")\n",
    "\n",
    "# print(df.info())\n",
    "\n",
    "missing_values = df.isna().sum()\n",
    "# print(f\"Missing values:\\n{missing_values[missing_values > 0]}\")  # Only prints columns with missing values\n",
    "\n",
    "# Transform feature from string to binary. Benign is now '0', and Malignant is '1'.\n",
    "df[\"diagnosis\"] = df[\"diagnosis\"].map({\"B\": 0, \"M\": 1})\n",
    "\n",
    "# print(df.iloc[0])\n",
    "\n",
    "# Select target attribute\n",
    "y = df[\"diagnosis\"]\n",
    "\n",
    "# Drop useless features\n",
    "X = df.drop([\"Unnamed: 32\", \"diagnosis\", \"id\"], axis=1)\n",
    "# print(df.columns)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Manual standardization\n",
    "# mean = np.mean(X, axis=0)  # Compute mean for each feature\n",
    "# std = np.std(X, axis=0)  # Compute standard deviation for each feature\n",
    "# # Avoid division by zero\n",
    "# std[std == 0] = 1  \n",
    "# X = (X - mean) / std  # Standardization formula\n",
    "\n",
    "# NOTE: Might be worth handling class imbalances somehow, currently it is 357(benign) to 212(malignant)\n",
    "# NOTE: Consider other model performance measurements than simply missclassification rate, maybe specificity(FNR). Consider ROC/AUC\n",
    "# NOTE: Set random states to make results reproducible\n",
    "# NOTE: Show confusion matrix?\n",
    "# NOTE: Is my kfold_evaluation a sensible way to evaluate bagged and boosted models?\n",
    "# NOTE: Include explainability of final model?\n",
    "pd.DataFrame(X).describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_full = PCA().fit(X)\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(np.cumsum(pca_full.explained_variance_ratio_), marker='o')\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Cumulative Explained Variance')\n",
    "# plt.grid(True)\n",
    "# plt.title('PCA - Explained Variance vs. Number of Components')\n",
    "# plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30)\n",
      "(569, 10)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=0.95).fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "# X_pca = pca.fit_transform(X)\n",
    "comp = pca.components_\n",
    "print(comp.shape)\n",
    "print(X_pca.shape)\n",
    "# print(X_pca.shape)\n",
    "\n",
    "# Check how many components were selected\n",
    "# print(f\"Number of components selected: {X_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.00001, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations=num_iterations\n",
    "        self.weights = None\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "        # return np.where(x >= 0, \n",
    "        #             1 / (1 + np.exp(-x)), \n",
    "        #             np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        for i in range(self.num_iterations):\n",
    "            p = self.sigmoid(X @ self.weights)\n",
    "            lossGradient = X.T @ (y - p) / num_samples\n",
    "            self.weights = self.weights + self.learning_rate * (lossGradient)\n",
    "            \n",
    "            # Print loss\n",
    "            # if i % 100 == 0:  # Print every 100 iterations\n",
    "            #     loss = -np.mean(y * np.log(p + 1e-15) + (1 - y) * np.log(1 - p + 1e-15))\n",
    "            #     print(f\"Iteration {i}: Loss = {loss:.4f}\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "        probs = self.sigmoid(X @ self.weights)\n",
    "        return np.column_stack([1 - probs, probs])  # Convert to (n_samples, 2) format\n",
    "\n",
    "    def predict(self, X, threshold=0.49864656):\n",
    "        # X = np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "        probabilities = self.predict_proba(X)[:,1]\n",
    "\n",
    "        # linear_combination = X @ self.weights\n",
    "        # activated = self.sigmoid(linear_combination)\n",
    "        # prediction = activated > threshold\n",
    "        return probabilities > threshold\n",
    "    \n",
    "class BaseKNN:\n",
    "    def __init__(self, X, y, K=5, distanceMeasure=\"Euclidean\"):\n",
    "        self.K = K\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.distanceMeasure = distanceMeasure\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def getDistance(self, x1, x2):\n",
    "        if self.distanceMeasure == \"Euclidean\":\n",
    "            # distance = np.sqrt(np.sum((x1 - x2)**2))\n",
    "            distance = np.linalg.norm(x1 - x2)\n",
    "        return distance\n",
    "\n",
    "    def get_K_NN(self, x):\n",
    "        distances = np.array([self.getDistance(row, x) for row in self.X])\n",
    "        K_neighbours_indices = np.argsort(distances)[:self.K]\n",
    "        NN_classes = self.y[K_neighbours_indices]\n",
    "        \n",
    "        return NN_classes\n",
    "    \n",
    "    def predict(self, X):\n",
    "        toPrint = [self.get_K_NN(x) for x in X]\n",
    "        toPrint = np.array(toPrint)\n",
    "        print(toPrint.shape)\n",
    "        predictions = [stats.mode(self.get_K_NN(x), keepdims=False).mode for x in X]\n",
    "        return np.array(predictions)\n",
    "        # K_NN = self.get_K_NN(x)\n",
    "        # # print(K_NN.shape)\n",
    "        # return stats.mode(K_NN)\n",
    "\n",
    "class BaseSoftKNN:\n",
    "    def __init__(self, X, y, K=5, distanceMeasure=\"Euclidean\"):\n",
    "        self.K = K\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.distanceMeasure = distanceMeasure\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def getDistance(self, x1, x2):\n",
    "        if self.distanceMeasure == \"Euclidean\":\n",
    "            # distance = np.sqrt(np.sum((x1 - x2)**2))\n",
    "            distance = np.linalg.norm(x1 - x2)\n",
    "        return distance\n",
    "\n",
    "    def get_K_NN(self, x):\n",
    "        distances = np.array([self.getDistance(row, x) for row in self.X])\n",
    "        K_neighbours_indices = np.argsort(distances)[:self.K]\n",
    "        NN_classes = self.y[K_neighbours_indices]\n",
    "        \n",
    "        return NN_classes\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        classes = [self.get_K_NN(x) for x in X]\n",
    "        probas = np.sum(np.array(classes), axis=1) / self.K\n",
    "        return np.column_stack([1 - probas, probas]) # Convert to (n_samples, 2) format\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        probas = self.predict_proba(X)\n",
    "        predictions = (probas > threshold).astype(int)\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_evaluation(K, model, X, y, threshold=0.49864656):\n",
    "    CV = model_selection.KFold(n_splits=K, shuffle=True)\n",
    "\n",
    "    all_y_true = []\n",
    "    all_y_prob = []\n",
    "\n",
    "    for train_index, test_index in CV.split(X):\n",
    "        X_train, y_train = X[train_index, :], y[train_index]\n",
    "        X_test, y_test = X[test_index, :], y[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        all_y_true.extend(y_test)\n",
    "        all_y_prob.extend(y_prob)\n",
    "\n",
    "    y_est_global = (np.array(all_y_prob) >= threshold).astype(int)\n",
    "    cm = confusion_matrix(all_y_true, y_est_global, labels=[0, 1])\n",
    "\n",
    "    fpr_roc, tpr_roc, thresholds = metrics.roc_curve(all_y_true, all_y_prob)\n",
    "    auc = metrics.auc(fpr_roc, tpr_roc)\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    error = (fp + fn) / len(all_y_true)\n",
    "    accuracy = 1 - error\n",
    "    f1_score = (2 * tp) / (2 * tp + fp + fn)\n",
    "    \n",
    "    return accuracy, error, f1_score, auc, fpr, tpr, fnr, tnr, precision\n",
    "\n",
    "def kfold_evaluation_outliers_removed(K, model, X, y, threshold=0.49864656):\n",
    "    CV = model_selection.KFold(n_splits=K, shuffle=True)\n",
    "\n",
    "    all_y_true = []\n",
    "    all_y_prob = []\n",
    "\n",
    "    for train_index, test_index in CV.split(X):\n",
    "        X_train, y_train = X[train_index, :], y[train_index]\n",
    "        X_test, y_test = X[test_index, :], y[test_index]\n",
    "        \n",
    "        # mean = np.mean(X_train, axis=0)\n",
    "        # std = np.std(X_train, axis=0)\n",
    "\n",
    "        non_outliers = np.all(np.abs(X_train) < 5, axis=1)\n",
    "\n",
    "        X_train_clean = X_train[non_outliers]\n",
    "        y_train_clean = y_train[non_outliers]\n",
    "\n",
    "        model.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        all_y_true.extend(y_test)\n",
    "        all_y_prob.extend(y_prob)\n",
    "\n",
    "    y_est_global = (np.array(all_y_prob) >= threshold).astype(int)\n",
    "    cm = confusion_matrix(all_y_true, y_est_global, labels=[0, 1])\n",
    "\n",
    "    fpr_roc, tpr_roc, thresholds = metrics.roc_curve(all_y_true, all_y_prob)\n",
    "    auc = metrics.auc(fpr_roc, tpr_roc)\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "        \n",
    "    error = (fp + fn) / len(all_y_true)\n",
    "    accuracy = 1 - error\n",
    "    f1_score = (2 * tp) / (2 * tp + fp + fn)\n",
    "    \n",
    "    return accuracy, error, f1_score, auc, fpr, tpr, fnr, tnr\n",
    "\n",
    "def kfold_accuracy(K, model, X, y, threshold=0.49864656):\n",
    "    CV = model_selection.KFold(n_splits=K, shuffle=True)\n",
    "\n",
    "    errors = np.zeros(K)\n",
    "    i = 0\n",
    "    for train_index, test_index in CV.split(X):\n",
    "        X_train, y_train = X[train_index, :], y[train_index]\n",
    "        X_test, y_test = X[test_index, :], y[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        fold_error = np.sum(y_pred != y_test) / len(y_test)\n",
    "        errors[i] = fold_error\n",
    "        i += 1\n",
    "\n",
    "        # y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "    \n",
    "    \n",
    "    # y_est_global = (np.array(all_y_prob) >= threshold).astype(int)\n",
    "    avg_error = np.mean(errors)\n",
    "        \n",
    "    # error = (fp + fn) / len(all_y_true)\n",
    "    accuracy = 1 - avg_error\n",
    "    \n",
    "    return accuracy, avg_error\n",
    "\n",
    "def get_kfold_roc(K, model, X, y):\n",
    "    CV = model_selection.KFold(n_splits=K, shuffle=True)\n",
    "    \n",
    "    all_y_true = []\n",
    "    all_y_prob = []\n",
    "\n",
    "    for train_index, test_index in CV.split(X):\n",
    "        X_train, y_train = X[train_index, :], y[train_index]\n",
    "        X_test, y_test = X[test_index, :], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # Get probability estimates\n",
    "\n",
    "        all_y_true.extend(y_test)\n",
    "        all_y_prob.extend(y_prob)\n",
    "\n",
    "    # Compute overall ROC\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(all_y_true, all_y_prob)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    return fpr, tpr, auc, thresholds\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, auc):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='grey', linestyle='--')  # Random guess line\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Graph and AUC Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = X.shape[0]\n",
    "K = 10\n",
    "fpr, tpr, auc, thresholds = get_kfold_roc(K, LogisticRegression(), X, y)\n",
    "# fpr2, tpr2, auc2, thresholds2 = get_kfold_roc(10, LogisticRegression(), X, y)\n",
    "\n",
    "# plot_roc_curve(fpr, tpr, auc)\n",
    "\n",
    "# for i in range(len(thresholds)):\n",
    "#     print(f\"Threshold: {thresholds[i]:.8f}, FPR: {fpr[i]:.8f}, TPR: {tpr[i]:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model accuracy: 0.9332161687170475\n",
      "Base model error: 0.06678383128295255\n",
      "Base f1 Score: 0.910377358490566\n",
      "Base model AUC: 0.9823608688758522\n",
      "Base model fnr: 0.08962264150943396\n",
      "Base model fpr: 0.05322128851540616\n",
      "\n",
      "Nonbase model accuracy: 0.984182776801406\n",
      "Nonbase model error: 0.015817223198594025\n",
      "NonBase f1 Score: 0.9785202863961814\n",
      "NonBase model AUC: 0.9950848263833835\n",
      "NonBase model fnr: 0.0330188679245283\n",
      "NonBase model fpr: 0.0056022408963585435\n",
      "NonBase Recall: 0.9669811320754716\n",
      "Nonbase Precision: 0.9903381642512077\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Compare base Logistic Regression to task Logistic Regression through K-fold cross validation\n",
    "K = X.shape[0]\n",
    "# K = 10\n",
    "base_accuracy, base_error, base_f1_score, base_auc, base_fpr, base_tpr, base_fnr, base_tnr, base_precision = kfold_evaluation(K, BaseLogisticRegression(num_iterations=1000), X, y, threshold=0.5)\n",
    "print(f\"Base model accuracy: {base_accuracy}\\nBase model error: {base_error}\\nBase f1 Score: {base_f1_score}\\nBase model AUC: {base_auc}\\nBase model fnr: {base_fnr}\\nBase model fpr: {base_fpr}\\n\")\n",
    "# nonbase_accuracy, nonbase_error, nonbase_f1_score, nonbase_auc, nonbase_fpr, nonbase_tpr, nonbase_fnr, nonbase_tnr = kfold_evaluation(K, LogisticRegression(max_iter=1000), X, y, threshold=0.49864656)\n",
    "nonbase_accuracy, nonbase_error, nonbase_f1_score, nonbase_auc, nonbase_fpr, nonbase_tpr, nonbase_fnr, nonbase_tnr, nonbase_precision = kfold_evaluation(K, LogisticRegression(C=0.55, max_iter=1000), X, y, threshold=0.49864656)\n",
    "print(f\"Nonbase model accuracy: {nonbase_accuracy}\\nNonbase model error: {nonbase_error}\\nNonBase f1 Score: {nonbase_f1_score}\\nNonBase model AUC: {nonbase_auc}\\nNonBase model fnr: {nonbase_fnr}\\nNonBase model fpr: {nonbase_fpr}\\nNonBase Recall: {nonbase_tpr}\\nNonbase Precision: {nonbase_precision}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.9314\n",
    "# 0.9824"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonbase model accuracy: 0.9718804920913884\n",
      "Nonbase model error: 0.028119507908611598\n",
      "NonBase f1 Score: 0.9615384615384616\n",
      "NonBase model AUC: 0.9893702764124517\n",
      "NonBase model fnr: 0.05660377358490566\n",
      "NonBase model fpr: 0.011204481792717087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K = X.shape[0]\n",
    "# K = 10\n",
    "# Compare base KNN to task KNN through K-fold cross validation\n",
    "# base_accuracy, base_error, base_f1_score, base_auc, base_fpr, base_tpr, base_fnr, base_tnr = kfold_evaluation(K, BaseSoftKNN(X, y, 5), X, y)\n",
    "nonbase_accuracy, nonbase_error, nonbase_f1_score, nonbase_auc, nonbase_fpr, nonbase_tpr, nonbase_fnr, nonbase_tnr = kfold_evaluation_outliers_removed(K, KNeighborsClassifier(n_neighbors=10), X, y)\n",
    "\n",
    "# print(f\"Base model accuracy: {base_accuracy}\\nBase model error: {base_error}\\nBase f1 Score: {base_f1_score}\\nBase model AUC: {base_auc}\\nBase model fnr: {base_fnr}\\nBase model fpr: {base_fpr}\\n\")\n",
    "print(f\"Nonbase model accuracy: {nonbase_accuracy}\\nNonbase model error: {nonbase_error}\\nNonBase f1 Score: {nonbase_f1_score}\\nNonBase model AUC: {nonbase_auc}\\nNonBase model fnr: {nonbase_fnr}\\nNonBase model fpr: {nonbase_fpr}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.9701230228471002\n",
      "Model error: 0.029876977152899824\n",
      "f1 Score: 0.9601873536299765\n",
      "Model AUC: 0.9951905290418055\n",
      "Model fnr: 0.0330188679245283\n",
      "Model fpr: 0.028011204481792718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K = X.shape[0]\n",
    "# accuracy, error = kfold_accuracy(K, svm.SVC(kernel=\"rbf\"), X, y)\n",
    "accuracy, error, f1_score, auc, fpr, tpr, fnr, tnr = kfold_evaluation_outliers_removed(K, svm.SVC(kernel=\"rbf\", probability=True), X, y)\n",
    "# model.fit(X,y)\n",
    "\n",
    "# print(accuracy, error)    \n",
    "print(f\"Model accuracy: {accuracy}\\nModel error: {error}\\nf1 Score: {f1_score}\\nModel AUC: {auc}\\nModel fnr: {fnr}\\nModel fpr: {fpr}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.9630931458699473\n",
      "Model error: 0.03690685413005272\n",
      "f1 Score: 0.9501187648456056\n",
      "Model AUC: 0.988815337455737\n",
      "Model fnr: 0.05660377358490566\n",
      "Model fpr: 0.025210084033613446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K = X.shape[0]\n",
    "# K = 10\n",
    "# model = BaggingClassifier(LogisticRegression(max_iter=1000))\n",
    "# accuracy, error = kfold_accuracy(K, BaggingClassifier(LogisticRegression()), X, y)\n",
    "# accuracy, error = kfold_accuracy(K, AdaBoostClassifier(LogisticRegression()), X, y)\n",
    "accuracy, error, f1_score, auc, fpr, tpr, fnr, tnr = kfold_evaluation_outliers_removed(K, RandomForestClassifier(), X, y)\n",
    "print(f\"Model accuracy: {accuracy}\\nModel error: {error}\\nf1 Score: {f1_score}\\nModel AUC: {auc}\\nModel fnr: {fnr}\\nModel fpr: {fpr}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(accuracy, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleSVM:\n",
    "    def __init__(self, kernel='linear', C=1.0, degree=3, gamma=None, learning_rate=0.01, n_iters=1000):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.alpha = None\n",
    "        self.b = 0\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def kernel_function(self, x1, x2):\n",
    "        if self.kernel == 'linear':\n",
    "            return x1 @ x2\n",
    "        elif self.kernel == 'poly':\n",
    "            return ((x1 @ x2) + 1)**self.degree\n",
    "        elif self.kernel == 'rbf':\n",
    "            if self.gamma is None:\n",
    "                self.gamma = 1 / x1.shape[0]   \n",
    "            return np.exp(-self.gamma * np.linalg.norm(x1 - x2)**2)\n",
    "        else: \n",
    "            raise ValueError(\"Unknown kernel\")\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = copy(X)\n",
    "        self.y_train = copy(y)\n",
    "        n_samples, _ = X.shape\n",
    "        self.alpha = np.zeros(n_samples)\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for i in range(n_samples):\n",
    "                gradient = 1 - y[i] * sum(self.alpha[j] * y[j] * self.kernel_function(X[i], X[j]) for j in range(n_samples))\n",
    "                self.alpha[i] += self.learning_rate * gradient\n",
    "                self.alpha[i] = min(max(self.alpha[i], 0), self.C)\n",
    "\n",
    "        self.b = 1 / X.shape[0] * sum(y[i] - sum(\n",
    "            self.alpha[j] * y[j] * self.kernel_function(X[j], X[i])\n",
    "            for j in range(n_samples)) \n",
    "            for i in range(n_samples))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = np.sign([\n",
    "            np.sum(\n",
    "                self.alpha * self.y_train * [self.kernel_function(x_i, x) for x_i in self.X_train] + self.b for x in X\n",
    "            )\n",
    "        ])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_accuracy, base_avg_error = kfold_accuracy(10, simpleSVM(kernel=\"rbf\"), X, y)\n",
    "\n",
    "# print(f\"Base SVM accuracy: {base_accuracy}\\nBase SVM error: {base_avg_error}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
